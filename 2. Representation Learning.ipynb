{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from models.answer_exists_models import *\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b9ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fi.300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (0.42%) [>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (0.51%) [>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]>                                                  ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasttext.util.download_model('fi', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55dcf289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_en.shape[0]):\n",
    "    question_tokens = train_en['question'].iloc[i]\n",
    "    \n",
    "    vecs = np.array([ft.get_word_vector(token) for token in question_tokens])\n",
    "    print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = ft.get_input_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3070a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000000, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d21c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = ft.get_output_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264c0aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c491d781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_id(\"[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7149ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaf5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_id(token, ft):\n",
    "    word_id = ft.get_word_id(token)\n",
    "    assert word_id != -1, \"OOV token found -> {}\".format(token)\n",
    "    return word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ab75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List, ft, max_len=512) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    #input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "    input_ids = [[check_word_id(token, ft) for token in tokens] for tokens in text]\n",
    "    \n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fe02825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[21180, 21180], [141], [1944]], [2, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_batch_bilstm([[\"sailor\", \"sailor\"], [\"where\"], [\"Where\"]], ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be845904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8627c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiLSTM network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "        \n",
    "        \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
    "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40decc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = ft.get_input_matrix()\n",
    "pretrained_embeddings = np.concatenate((np.zeros((1,pretrained_embeddings.shape[1])),pretrained_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfd16d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BiLSTMNetwork(\n\u001b[0;32m----> 3\u001b[0m     pretrained_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mpretrained_embeddings\u001b[49m), \n\u001b[1;32m      4\u001b[0m     lstm_dim\u001b[38;5;241m=\u001b[39mlstm_dim, \n\u001b[1;32m      5\u001b[0m     dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m      6\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m   )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BiLSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=0.1, \n",
    "    n_classes=2\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31058589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
