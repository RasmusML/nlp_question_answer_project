{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import compress_fasttext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ea285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0921e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the question is answered, then predict \"1\". Otherwise predict \"0\".\n",
    "def get_target(data):\n",
    "    answer_set = data['document_answer_region']\n",
    "    y = np.empty(answer_set.shape[0], dtype=np.int32)\n",
    "\n",
    "    for i, answer in enumerate(answer_set):\n",
    "        if type(answer) == Annotation_error and answer == Annotation_error.UNANSWERED: # @TODO: if we don't do the annotation stuff, then we can check for -1 here\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9d2e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token, tok2vec):\n",
    "    assert OOV_id != None\n",
    "\n",
    "    try:\n",
    "        id = tok2vec.get_index(token)\n",
    "    except:\n",
    "        id = OOV_id # OOV\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4153ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(question_tokens, document_tokens, tok2vec, question_max_length=512, document_max_length=1024):\n",
    "    question_ids = [token_to_id(token, tok2vec) for token in question_tokens[:question_max_length]]\n",
    "    document_ids = [token_to_id(token, tok2vec) for token in document_tokens[:document_max_length]]\n",
    "    return question_ids, len(question_ids), document_ids, len(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "25a30d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(token_ids, token_lengths): # in a batch\n",
    "    assert pad_id != None\n",
    "    \n",
    "    max_length = max(token_lengths)\n",
    "    padded_ids = [(ids + [pad_id] * (max_length - len(ids))) for ids in token_ids]\n",
    "    \n",
    "    assert (all(len(padded) == max_length for padded in padded_ids))\n",
    "    \n",
    "    return padded_ids\n",
    "\n",
    "def collate_into_batch(raw_batch):\n",
    "    question_tokens = [sample[0] for sample in raw_batch]\n",
    "    question_lengths = [sample[1] for sample in raw_batch]\n",
    "    question_ids = pad_tokens(question_tokens, question_lengths)\n",
    "    \n",
    "    document_tokens = [sample[2] for sample in raw_batch]\n",
    "    document_lengths = [sample[3] for sample in raw_batch]\n",
    "    document_ids = pad_tokens(document_tokens, document_lengths)\n",
    "    \n",
    "    targets = [sample[4] for sample in raw_batch]\n",
    "\n",
    "    return torch.tensor(question_ids), torch.tensor(question_lengths), torch.tensor(document_ids), torch.tensor(document_lengths), torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "10bbee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweredDatasetReader(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tok2vec):\n",
    "        self.df = df\n",
    "        self.tok2vec = tok2vec\n",
    "        self.targets = get_target(df)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.values[idx]\n",
    "        question_tokens = row[1]\n",
    "        document_tokens = row[3]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        question_ids, question_lengths, document_ids, document_lengths = prepare_sample(question_tokens, document_tokens, self.tok2vec)\n",
    "        \n",
    "        return question_ids, question_lengths, document_ids, document_lengths, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a4b126b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 100, n_classes: int = 2):\n",
    "        super(LSTM_Network, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(2*lstm_dim, n_classes)\n",
    "\n",
    "    def forward(self, q_ids, q_lengths, d_ids, d_lengths):\n",
    "        q_embeds = self.word_embeddings(q_ids)\n",
    "        lstm_out, _ = self.lstm(q_embeds)\n",
    "        q_ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        d_embeds = self.word_embeddings(d_ids)\n",
    "        lstm_out, _ = self.lstm(d_embeds)\n",
    "        d_ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        ff_in = torch.cat((q_ff_in, d_ff_in), dim=-1)\n",
    "        \n",
    "        logits = self.linear(ff_in)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "694a4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 100, n_classes: int = 2):\n",
    "        super(BiLSTM_Network, self).__init__()\n",
    "\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1)\n",
    "        self.biLSTM = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*2*lstm_dim, n_classes)\n",
    "        \n",
    "    def forward(self, q_ids, q_lengths, d_ids, d_lengths):\n",
    "        q_embeds = self.embedding(q_ids)\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(q_embeds, q_lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        q_ff_in = torch.max(lstm_out, 1)[0]\n",
    "\n",
    "        d_embeds = self.embedding(d_ids)\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(d_embeds, d_lengths, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        d_ff_in = torch.max(lstm_out, 1)[0]\n",
    "\n",
    "        ff_in = torch.cat((q_ff_in, d_ff_in), dim=-1)\n",
    "        \n",
    "        logits = self.linear(ff_in)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "985c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_matrix(embed_matrix):\n",
    "    embed_dim = embed_matrix.shape[1]\n",
    "    pad = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    OOV = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    embed_pre = torch.tensor(embed_matrix, dtype=torch.float32)\n",
    "    \n",
    "    OOV_id = embed_pre.shape[0]           \n",
    "    pad_id = embed_pre.shape[0] + 1\n",
    "                 \n",
    "    return torch.vstack((embed_pre, pad, OOV)), pad_id, OOV_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "721fe0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "cd5d9bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "fcfbc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Network(\n",
      "  (embedding): Embedding(20002, 300, padding_idx=20001)\n",
      "  (biLSTM): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=400, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Setup Model\n",
    "\n",
    "tok2vec = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"fasttext-en-mini\")\n",
    "pretrained_embeddings, pad_id, OOV_id = prepare_embedding_matrix(tok2vec.get_normed_vectors())\n",
    "\n",
    "model = BiLSTM_Network(pretrained_embeddings)\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "8284eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(model: nn.Module):\n",
    "    \"\"\"\n",
    "    A simple function to quickly debug the net\n",
    "    \"\"\"\n",
    "    # Test the forward pass with dummy data\n",
    "    q = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "    q_lengths = torch.tensor([3], dtype=torch.int)\n",
    "    d = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "    d_lengths = torch.tensor([3], dtype=torch.int)\n",
    "\n",
    "    out = model.forward(q, q_lengths, d, d_lengths)\n",
    "\n",
    "    print(\"Output shape:\", out.size())\n",
    "    print(f\"Output probabilities:\\n{out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "433ccaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2])\n",
      "Output probabilities:\n",
      "[[-0.05225409  0.05980343]]\n"
     ]
    }
   ],
   "source": [
    "test_forward_pass(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "73f64d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_batch(token_ids, sequence_lengths):\n",
    "    actual_length = torch.sum(sequence_lengths)\n",
    "    total_length = np.sum([len(question) for question in token_ids])\n",
    "    \n",
    "    OOV_count = np.sum([torch.sum(question == OOV_id) for question in token_ids])\n",
    "    print(\"OOV: {}/{}\".format(OOV_count, total_length))\n",
    "    \n",
    "    pad_count = np.sum([torch.sum(question == pad_id) for question in token_ids])\n",
    "    print(\"PAD: {}/{}\".format(pad_count, total_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "1a736b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
    "    model.eval()\n",
    "\n",
    "    logits_all = []\n",
    "    targets_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for q_ids, q_lengths, d_ids, d_lengths, targets in valid_dl:\n",
    "            logits = model(q_ids, q_lengths, d_ids, d_lengths)\n",
    "            targets_all.extend(list(targets.detach().cpu().numpy()))\n",
    "            \n",
    "            predictions = logits.max(1)[1]\n",
    "            logits_all.extend(predictions)\n",
    "            \n",
    "        acc = accuracy_score(logits_all, targets_all)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "0e22e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=1/5 | loss=0.55, accuracy=0.76: 100%|██████████████████████████████████████████| 116/116 [20:45<00:00, 10.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/5 | loss=0.66, train_accuracy=0.60, test_accuracy=0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=2/5 | loss=0.40, accuracy=0.86: 100%|██████████████████████████████████████████| 116/116 [21:02<00:00, 10.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2/5 | loss=0.58, train_accuracy=0.71, test_accuracy=0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=3/5 | loss=0.42, accuracy=0.83: 100%|██████████████████████████████████████████| 116/116 [20:49<00:00, 10.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3/5 | loss=0.51, train_accuracy=0.76, test_accuracy=0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=4/5 | loss=0.37, accuracy=0.86: 100%|██████████████████████████████████████████| 116/116 [20:51<00:00, 10.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4/5 | loss=0.47, train_accuracy=0.78, test_accuracy=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=5/5 | loss=0.47, accuracy=0.79: 100%|██████████████████████████████████████████| 116/116 [20:33<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5/5 | loss=0.46, train_accuracy=0.79, test_accuracy=0.80\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Define hyper parameters\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters()) #, weight_decay=1e-6\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = QuestionAnsweredDatasetReader(train_en, tok2vec)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_into_batch)\n",
    "\n",
    "val_dataset = QuestionAnsweredDatasetReader(validation_en, tok2vec)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_into_batch)\n",
    "\n",
    "# store improvement per epoch\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    ### Training\n",
    "    model.train()\n",
    "    \n",
    "    # Store batch loss and accuracy\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    \n",
    "    batch_pbar = tqdm(train_dataloader)\n",
    "    for q_ids, q_lengths, d_ids, d_lengths, targets in batch_pbar:\n",
    "        \n",
    "        # training\n",
    "        outputs = model(q_ids, q_lengths, d_ids, d_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # prediction\n",
    "        predictions = outputs.max(1)[1]\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        \n",
    "        loss_epoch.append(loss.item())\n",
    "        accuracy_epoch.append(accuracy)\n",
    "        \n",
    "        batch_pbar.set_description(f\"epoch={epoch+1}/{n_epochs} | loss={loss.item():.2f}, accuracy={accuracy:.2f}\")\n",
    "        \n",
    "    train_loss = np.mean(loss_epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    train_acc = np.mean(accuracy_epoch)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    ### Evaluation\n",
    "    test_acc = evaluate(model, train_dataloader)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"epoch={epoch+1}/{n_epochs} | loss={train_loss:.2f}, train_accuracy={train_acc:.2f}, test_accuracy={test_acc:.2f}\")\n",
    "    \n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "905088cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.set_title(\"BiLSTM model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "epoch_range = np.arange(n_epochs, step=1)\n",
    "ax.plot(epoch_range, train_accuracies)\n",
    "ax.plot(epoch_range, test_accuracies)\n",
    "ax.legend(['Train Accuracy','Validation Accuracy'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "ax.set_yticks(np.linspace(0.5, 1, 11))\n",
    "ax.set_xticks(epoch_range);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "06bb357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.set_title(\"LSTM model\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "epoch_range = np.arange(n_epochs, step=1)\n",
    "ax.plot(epoch_range, train_accuracies)\n",
    "ax.plot(epoch_range, test_accuracies)\n",
    "ax.legend(['Train Accuracy','Validation Accuracy'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "ax.set_yticks(np.linspace(0.5, 1, 11))\n",
    "ax.set_xticks(epoch_range);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8153295516967773, 'cupcake'),\n",
       " (0.7879170179367065, 'cookie-'),\n",
       " (0.7691606879234314, 'cookies'),\n",
       " (0.760718047618866, 'cake'),\n",
       " (0.7547473907470703, 'popover'),\n",
       " (0.7430337071418762, 'non-cookie'),\n",
       " (0.7359125018119812, 'cakepop'),\n",
       " (0.7353752255439758, 'muffin'),\n",
       " (0.7349858283996582, 'cookie.'),\n",
       " (0.7314939498901367, 'cookie.The')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "627a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31058589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> confection.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], confection.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], enable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      enable (Iterable[str]): Names of pipeline components to enable. All other\n",
      " |          pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import compress_fasttext\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from compress_fasttext.feature_extraction import FastTextTransformer\n",
    "\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-en-mini')\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    FastTextTransformer(model=small_model), \n",
    "    LogisticRegression()\n",
    ").fit(\n",
    "    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],\n",
    "    [1, 1, 1, 0, 0, 0]\n",
    ")\n",
    "classifier.predict(['jet', 'train', 'cake', 'apple'])\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b97420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "449e1dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c219863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15784"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_index(\"slap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3356762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quantum', 0.5923356945161),\n",
       " ('physics', 0.4987263608889812),\n",
       " ('computational', 0.4833229306372649),\n",
       " ('cosmic', 0.46287664812730667),\n",
       " ('atomic', 0.4555259364535978),\n",
       " ('atoms', 0.4543258391303013),\n",
       " ('electron', 0.4415847215404407),\n",
       " ('electromagnetic', 0.4342020722504953),\n",
       " ('optical', 0.4341506741975586),\n",
       " ('physicist', 0.43388256332181196)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.most_similar(\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af44447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model[\"sailor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78032967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tok2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00265f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
