{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from models.answer_exists_models import *\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import compress_fasttext\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5430bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: compress-fasttext in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: gensim>=4.0.0 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (4.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (1.23.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (1.9.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install compress-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ea285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0921e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the question is answered, then predict \"1\". Otherwise predict \"0\".\n",
    "def get_target(data):\n",
    "    answer_set = data['document_answer_region']\n",
    "    y = np.empty(answer_set.shape[0], dtype=np.int32)\n",
    "\n",
    "    for i, answer in enumerate(answer_set):\n",
    "        if type(answer) == Annotation_error and answer == Annotation_error.UNANSWERED: # @TODO: if we don't do the annotation stuff, then we can check for -1 here\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ad0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(raw_batch): #-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    global pad_id # @TODO: cleanup\n",
    "    \n",
    "    targets = [sample[2] for sample in raw_batch]\n",
    "    sequence_lengths = [sample[1] for sample in raw_batch]\n",
    "    \n",
    "    max_length = max(sequence_lengths)\n",
    "    \n",
    "    # Pad all of the input samples to the max length\n",
    "    token_ids = [(sample[0] + [pad_id] * (max_length - len(sample[0]))) for sample in raw_batch]\n",
    "    \n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in token_ids))\n",
    "    \n",
    "    return torch.tensor(token_ids), torch.tensor(sequence_lengths), torch.tensor(targets, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token, tok2vec):\n",
    "    global OOV_id # @TODO: cleanup\n",
    "    \n",
    "    try:\n",
    "        id = tok2vec.get_index(token)\n",
    "    except:\n",
    "        id = OOV_id # OOV\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4153ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(question_tokens, tok2vec, max_length=512):\n",
    "    token_ids = [token_to_id(token, tok2vec) for token in question_tokens[:max_length]]\n",
    "    return token_ids, len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bbee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweredDatasetReader(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tok2vec):\n",
    "        self.df = df\n",
    "        self.tok2vec = tok2vec\n",
    "        self.targets = get_target(df)\n",
    "        \n",
    "        #print(np.sum(self.targets == 0), np.sum(self.targets==1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.values[idx]\n",
    "        question_tokens = row[1]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        token_ids, seq_length = prepare_sample(question_tokens, self.tok2vec)\n",
    "        \n",
    "        return token_ids, seq_length, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83241085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, n_classes: int = 2):\n",
    "        super(TinyNetwork1, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(pretrained_embeddings),\n",
    "            nn.Linear(pretrained_embeddings.shape[1], n_classes),\n",
    "            #nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        return self.net.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6515fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork2(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 20, n_hidden: int = 5, n_classes: int = 2):\n",
    "        super(TinyNetwork2, self).__init__()\n",
    "\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1)\n",
    "        self.biLSTM = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 1, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*lstm_dim, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "        \n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.biLSTM.named_parameters()) + list(self.linear1.named_parameters()) + list(self.linear2.named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # b x sl x emb_dim\n",
    "        embeds = self.embedding(inputs)\n",
    "    \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(embeds, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence (b x sl x 2*lstm_dim)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # representation of the last lstm unit (b x 2*lstm_dim)\n",
    "        n_batch = lstm_out.shape[0]\n",
    "        input_ends = input_lengths\n",
    "        #ff_in = torch.vstack([lstm_out[i, input_ends[i]-1, :] for i in range(n_batch)])\n",
    "        # Alternative: Max element-wise over all hidden units\n",
    "        ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        # (b x n_hidden)\n",
    "        x = self.linear1(ff_in)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # (b x n_classes)\n",
    "        logits = self.linear2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "92336d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1: [.1, .4, .6], [.!, .!, .!], [.0, .0, .0], [.0, .0, .0],\n",
    "# b2: [.1, .4, .6], [.1, .4, .6], [.!, .!, .!], [.0, .0, .0],\n",
    "# b3: [.1, .4, .6], [.1, .4, .6], [.1, .4, .6], [.!, .!, .!],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7937910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2],[2,3],[6,4],[4,5]], [[5,6],[6,7],[7,8],[8,9]]])\n",
    "indices = [0, 2]\n",
    "torch.vstack([a[i,indices[i],:] for i in range(len(indices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "985c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_matrix(embed_matrix):\n",
    "    embed_dim = embed_matrix.shape[1]\n",
    "    pad = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    OOV = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    embed_pre = (torch.tensor(embed_matrix, dtype=torch.float32))\n",
    "    \n",
    "    OOV_id = embed_pre.shape[0]           \n",
    "    pad_id = embed_pre.shape[0] + 1\n",
    "                 \n",
    "    return torch.vstack((embed_pre, pad, OOV)), pad_id, OOV_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "721fe0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd5d9bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fcfbc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNetwork1(\n",
      "  (net): Sequential(\n",
      "    (0): Embedding(20002, 300)\n",
      "    (1): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tok2vec = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"fasttext-en-mini\")\n",
    "pretrained_embeddings, pad_id, OOV_id = prepare_embedding_matrix(tok2vec.get_normed_vectors())\n",
    "\n",
    "model = TinyNetwork1(pretrained_embeddings)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "433ccaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 4, 2])\n",
      "Output probabilities:\n",
      "[[[0.03752185 0.03206366]\n",
      "  [0.04019786 0.05317501]\n",
      "  [0.05897662 0.05533287]\n",
      "  [0.04542413 0.07715003]]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "inputs = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "input_lengths = torch.tensor([3], dtype=torch.int)\n",
    "out = model.forward(inputs, input_lengths)\n",
    "\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output probabilities:\\n{out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4699ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) #, weight_decay=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73f64d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(token_ids, sequence_lengths):\n",
    "    actual_length = torch.sum(sequence_lengths)\n",
    "    total_length = np.sum([len(question) for question in token_ids])\n",
    "    \n",
    "    OOV_count = np.sum([torch.sum(question == OOV_id) for question in token_ids])\n",
    "    print(\"OOV: {}/{}\".format(OOV_count, total_length))\n",
    "    \n",
    "    pad_count = np.sum([torch.sum(question == pad_id) for question in token_ids])\n",
    "    print(\"PAD: {}/{}\".format(pad_count, total_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4bfef0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_dummy = train_en.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1fbf5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ok(start, end, ok, data):\n",
    "    for i in range(start, min(end, data.shape[0])):\n",
    "        data.at[i, 'question'] = [ok]\n",
    "        data.at[i, 'document_answer_region'] = (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1ac0f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bad(start, end, bad, data):\n",
    "    for i in range(start, min(end, data.shape[0])):\n",
    "        data.at[i, 'question'] = [bad]\n",
    "        data.at[i, 'document_answer_region'] = Annotation_error.UNANSWERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9278bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ok(0, 1000, \"apple\", train_en_dummy)\n",
    "create_bad(1000, 2000, \"dog\", train_en_dummy)\n",
    "create_ok(2000, 3000, \"orange\", train_en_dummy)\n",
    "create_bad(3000, 4000, \"horse\", train_en_dummy)\n",
    "create_ok(5000, 6000, \"car\", train_en_dummy)\n",
    "create_ok(6000, 10000, \"plane\", train_en_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5493f7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Koji, Kondo]</td>\n",
       "      <td>[Koji, Kondo(, 近藤, 浩治, ,, Kondō, Kōji, ,, born...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Czech, National, Symphony, Orchestra]</td>\n",
       "      <td>[The, Czech, National, Symphony, Orchestra, (,...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[List, of, tallest, buildings, and, structures]</td>\n",
       "      <td>[The, world, 's, tallest, artificial, structur...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Montague, Island, Light]</td>\n",
       "      <td>[The, Montague, Island, Light, is, an, heritag...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[List, of, United, States, military, bases]</td>\n",
       "      <td>[The, United, States, is, the, largest, operat...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Amstrad, CPC]</td>\n",
       "      <td>[The, CPC, models, ', hardware, is, based, on,...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[1960, South, African, republic, referendum]</td>\n",
       "      <td>[A, referendum, on, becoming, a, republic, was...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Nausicaä, of, the, Valley, of, the, Wind, (, ...</td>\n",
       "      <td>[Nausicaä, of, the, Valley, of, the, Wind, (, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Pixar]</td>\n",
       "      <td>[Pixar, Animation, Studios, ,, commonly, refer...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>english</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[Dodger, Stadium]</td>\n",
       "      <td>[In, order, to, comply, with, a, conditional, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language question                                     document_title  \\\n",
       "1000  english    [dog]                                      [Koji, Kondo]   \n",
       "1001  english    [dog]             [Czech, National, Symphony, Orchestra]   \n",
       "1002  english    [dog]    [List, of, tallest, buildings, and, structures]   \n",
       "1003  english    [dog]                          [Montague, Island, Light]   \n",
       "1004  english    [dog]        [List, of, United, States, military, bases]   \n",
       "1005  english    [dog]                                     [Amstrad, CPC]   \n",
       "1006  english    [dog]       [1960, South, African, republic, referendum]   \n",
       "1007  english    [dog]  [Nausicaä, of, the, Valley, of, the, Wind, (, ...   \n",
       "1008  english    [dog]                                            [Pixar]   \n",
       "1009  english    [dog]                                  [Dodger, Stadium]   \n",
       "\n",
       "                                               document  \\\n",
       "1000  [Koji, Kondo(, 近藤, 浩治, ,, Kondō, Kōji, ,, born...   \n",
       "1001  [The, Czech, National, Symphony, Orchestra, (,...   \n",
       "1002  [The, world, 's, tallest, artificial, structur...   \n",
       "1003  [The, Montague, Island, Light, is, an, heritag...   \n",
       "1004  [The, United, States, is, the, largest, operat...   \n",
       "1005  [The, CPC, models, ', hardware, is, based, on,...   \n",
       "1006  [A, referendum, on, becoming, a, republic, was...   \n",
       "1007  [Nausicaä, of, the, Valley, of, the, Wind, (, ...   \n",
       "1008  [Pixar, Animation, Studios, ,, commonly, refer...   \n",
       "1009  [In, order, to, comply, with, a, conditional, ...   \n",
       "\n",
       "           document_answer_region  \n",
       "1000  Annotation_error.UNANSWERED  \n",
       "1001  Annotation_error.UNANSWERED  \n",
       "1002  Annotation_error.UNANSWERED  \n",
       "1003  Annotation_error.UNANSWERED  \n",
       "1004  Annotation_error.UNANSWERED  \n",
       "1005  Annotation_error.UNANSWERED  \n",
       "1006  Annotation_error.UNANSWERED  \n",
       "1007  Annotation_error.UNANSWERED  \n",
       "1008  Annotation_error.UNANSWERED  \n",
       "1009  Annotation_error.UNANSWERED  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en_dummy[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff983933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e22e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 2], got [64]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [95], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(token_ids, sequence_lengths)\n\u001b[1;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m train_loss_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Backward pass.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ku_nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ku_nlp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ku_nlp\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [64, 2], got [64]"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "# train_en[2300:5000]\n",
    "train_dataset = QuestionAnsweredDatasetReader(train_en, tok2vec)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_dataset = QuestionAnsweredDatasetReader(validation_en, tok2vec)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "    train_acc_epoch = []\n",
    "    train_loss_epoch = []\n",
    "    \n",
    "    for token_ids, sequence_lengths, targets in train_dataloader:\n",
    "        model.train()\n",
    "        \n",
    "        #print(token_ids)\n",
    "        #print(targets)\n",
    "        \n",
    "        #print(\"==========================\")\n",
    "        #summarize(token_ids, sequence_lengths)\n",
    "        #print(token_ids[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass.\n",
    "        outputs = model(token_ids, sequence_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        train_loss_epoch.append(loss.detach().numpy())\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        #print(model.linear1.weight.grad)\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Do prediction\n",
    "        predictions = outputs.max(1)[1]\n",
    "        #print(predictions)\n",
    "        #print(targets)\n",
    "\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        train_acc_epoch.append(accuracy)\n",
    "        \n",
    "    accuracy = np.mean(train_acc_epoch)\n",
    "    m_loss = np.mean(train_loss_epoch)\n",
    "    \n",
    "    train_acc.append(accuracy)\n",
    "    train_loss.append(m_loss)\n",
    "    \n",
    "    print(\"Training | Accuracy: {:.2f}, Loss: {:.2f}\".format(accuracy, m_loss))\n",
    "        \n",
    "        \n",
    "    ### Evaluate validation\n",
    "    valid_acc_epoch = []\n",
    "    for token_ids, sequence_lengths, targets in val_dataloader:\n",
    "        model.eval()\n",
    "        \n",
    "        # Forward pass.\n",
    "        outputs = model(token_ids, sequence_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Do prediction\n",
    "        predictions = outputs.max(1)[1]\n",
    "\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        valid_acc_epoch.append(accuracy)\n",
    "        \n",
    "    accuracy = np.mean(valid_acc_epoch)\n",
    "    val_acc.append(accuracy)\n",
    "    \n",
    "    print(\"Validiation | Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "586efe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Updates'), Text(0, 0.5, 'Acc'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, val_acc)\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41f03be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a5ab723",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tok2vec.get_index(\"boat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fd334d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.6974e-03,  4.3535e-05]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor([[a]]), torch.tensor([1], dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8153295516967773, 'cupcake'),\n",
       " (0.7879170179367065, 'cookie-'),\n",
       " (0.7691606879234314, 'cookies'),\n",
       " (0.760718047618866, 'cake'),\n",
       " (0.7547473907470703, 'popover'),\n",
       " (0.7430337071418762, 'non-cookie'),\n",
       " (0.7359125018119812, 'cakepop'),\n",
       " (0.7353752255439758, 'muffin'),\n",
       " (0.7349858283996582, 'cookie.'),\n",
       " (0.7314939498901367, 'cookie.The')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "627a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31058589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> confection.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], confection.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], enable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      enable (Iterable[str]): Names of pipeline components to enable. All other\n",
      " |          pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import compress_fasttext\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from compress_fasttext.feature_extraction import FastTextTransformer\n",
    "\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-en-mini')\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    FastTextTransformer(model=small_model), \n",
    "    LogisticRegression()\n",
    ").fit(\n",
    "    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],\n",
    "    [1, 1, 1, 0, 0, 0]\n",
    ")\n",
    "classifier.predict(['jet', 'train', 'cake', 'apple'])\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b97420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "449e1dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c219863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15784"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_index(\"slap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3356762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quantum', 0.5923356945161),\n",
       " ('physics', 0.4987263608889812),\n",
       " ('computational', 0.4833229306372649),\n",
       " ('cosmic', 0.46287664812730667),\n",
       " ('atomic', 0.4555259364535978),\n",
       " ('atoms', 0.4543258391303013),\n",
       " ('electron', 0.4415847215404407),\n",
       " ('electromagnetic', 0.4342020722504953),\n",
       " ('optical', 0.4341506741975586),\n",
       " ('physicist', 0.43388256332181196)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.most_similar(\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af44447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model[\"sailor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78032967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tok2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00265f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
