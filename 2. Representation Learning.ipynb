{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from models.answer_exists_models import *\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ac9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting compress-fasttext\n",
      "  Downloading compress-fasttext-0.1.3.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gensim>=4.0.0\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (1.23.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (1.9.1)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "     -------------------------------------- 983.8/983.8 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (5.2.1)\n",
      "Building wheels for collected packages: compress-fasttext\n",
      "  Building wheel for compress-fasttext (setup.py): started\n",
      "  Building wheel for compress-fasttext (setup.py): finished with status 'done'\n",
      "  Created wheel for compress-fasttext: filename=compress_fasttext-0.1.3-py3-none-any.whl size=14583 sha256=6baef1fcf882eef53ebae0c1d8679020ee9826de74df8d8ef799cc3c3b06f81a\n",
      "  Stored in directory: c:\\users\\rasmu\\appdata\\local\\pip\\cache\\wheels\\c7\\63\\9f\\39db0410175167cee5eeae4fde2405d957cd05c1d8811a51cf\n",
      "Successfully built compress-fasttext\n",
      "Installing collected packages: Cython, gensim, compress-fasttext\n",
      "Successfully installed Cython-0.29.28 compress-fasttext-0.1.3 gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install compress-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "648b2f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfda5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the question is answered, then predict \"1\". Otherwise predict \"0\".\n",
    "def get_target(data):\n",
    "    answer_set = data['document_answer_region']\n",
    "    y = np.empty(answer_set.shape[0])\n",
    "\n",
    "    for i, answer in enumerate(answer_set):\n",
    "        if type(answer) == Annotation_error and answer == Annotation_error.UNANSWERED: # @TODO: if we don't do the annotation stuff, then we can check for -1 here\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e860674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import compress_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0060af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2vec = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"fasttext-en-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2d58e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<compress_fasttext.compress.CompressedFastTextKeyedVectors at 0x1b81c7209d0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e52355a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = words_to_ids(train_en['question'])\n",
    "lengths = [len(question_tokens) for question_tokens in train_en['question']]\n",
    "max_length = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18e123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01ddb9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tok2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.empty()\n",
    "for i in rangetrain_en['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d29f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_target(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d98a084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b3c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c34fb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "613240e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, n_classes: int = 2):\n",
    "        super(TinyNetwork, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(pretrained_embeddings),\n",
    "            nn.Linear(pretrained_embeddings.shape[1], n_classes),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.net.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "607f73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork2(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 6, n_classes: int = 2):\n",
    "        super(TinyNetwork2, self).__init__()\n",
    "\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.biLSTM = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*lstm_dim, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # b x sl x emb_dim\n",
    "        embeds = self.embedding(inputs)\n",
    "    \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(embeds, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence (b x sl x 2*lstm_dim)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # representation of the last lstm unit (b x 2*lstm_dim)\n",
    "        n_batch = lstm_out.shape[0]\n",
    "        input_ends = input_lengths-1\n",
    "        ff_in = torch.vstack([lstm_out[i, input_ends[i], :] for i in range(n_batch)])\n",
    "        # Alternative: Max element-wise over all hidden units\n",
    "        # ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.linear(ff_in)\n",
    "        \n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "5d4a4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1: [.1, .4, .6], [.!, .!, .!], [.0, .0, .0], [.0, .0, .0],\n",
    "# b2: [.1, .4, .6], [.1, .4, .6], [.!, .!, .!], [.0, .0, .0],\n",
    "# b3: [.1, .4, .6], [.1, .4, .6], [.1, .4, .6], [.!, .!, .!],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "932c9f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2],[2,3],[6,4],[4,5]], [[5,6],[6,7],[7,8],[8,9]]])\n",
    "indices = [0, 2]\n",
    "torch.vstack([a[i,indices[i],:] for i in range(len(indices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "cdb9a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNetwork2(\n",
      "  (embedding): Embedding(20000, 300)\n",
      "  (biLSTM): LSTM(300, 6, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = torch.tensor(tok2vec.get_normed_vectors(), dtype=torch.float32)\n",
    "model = TinyNetwork2(pretrained_embeddings)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "838ffaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2])\n",
      "Output probabilities:\n",
      "[[0.5093861  0.49061394]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "inputs = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "input_lengths = torch.tensor([3], dtype=torch.int)\n",
    "out = model.forward(inputs, input_lengths)\n",
    "\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output probabilities:\\n{out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "c6c8a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "902ab9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_sentences = torch.tensor([[42, 1, 0], [42, 1, 0], [18, 14, 0], [18, 1, 2]], dtype=torch.int32)\n",
    "train_lengths = torch.tensor([2, 2, 2, 3], dtype=torch.int32)\n",
    "train_target = torch.tensor([1, 1, 0, 0])\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    inputs = train_sentences\n",
    "    lengths = train_lengths\n",
    "    targets = train_target\n",
    "    \n",
    "    # Forward pass.\n",
    "    outputs = model(inputs, lengths)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # Backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    # Compute accuracy.\n",
    "    predictions = outputs.max(1)[1]\n",
    "    \n",
    "    #print(outputs)\n",
    "    #print(predictions)\n",
    "    \n",
    "    print(\"accuracy: {}\".format(accuracy_score(targets, predictions)))\n",
    "        \n",
    "\n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7149ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebaf5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_id(token, ft):\n",
    "    word_id = ft.get_word_id(token)\n",
    "    assert word_id != -1, \"OOV token found -> {}\".format(token)\n",
    "    return word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ab75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List, ft, max_len=512) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    #input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "    input_ids = [[check_word_id(token, ft) for token in tokens] for tokens in text]\n",
    "    \n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fe02825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[21180, 21180], [141], [1944]], [2, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_batch_bilstm([[\"sailor\", \"sailor\"], [\"where\"], [\"Where\"]], ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be845904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8627c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiLSTM network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "        \n",
    "        \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
    "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d40decc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = np.concatenate((np.zeros((1,pretrained_embeddings.shape[1])),pretrained_embeddings), axis=0)\n",
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfd16d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BiLSTMNetwork(\n\u001b[0;32m----> 3\u001b[0m     pretrained_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mpretrained_embeddings\u001b[49m), \n\u001b[1;32m      4\u001b[0m     lstm_dim\u001b[38;5;241m=\u001b[39mlstm_dim, \n\u001b[1;32m      5\u001b[0m     dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m      6\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m   )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BiLSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=0.1, \n",
    "    n_classes=2\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31058589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> confection.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], confection.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], enable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      enable (Iterable[str]): Names of pipeline components to enable. All other\n",
      " |          pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import compress_fasttext\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from compress_fasttext.feature_extraction import FastTextTransformer\n",
    "\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-en-mini')\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    FastTextTransformer(model=small_model), \n",
    "    LogisticRegression()\n",
    ").fit(\n",
    "    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],\n",
    "    [1, 1, 1, 0, 0, 0]\n",
    ")\n",
    "classifier.predict(['jet', 'train', 'cake', 'apple'])\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "170fbe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d9a6444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e69c7419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15784"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_index(\"slap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0d6abf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quantum', 0.5923356945161),\n",
       " ('physics', 0.4987263608889812),\n",
       " ('computational', 0.4833229306372649),\n",
       " ('cosmic', 0.46287664812730667),\n",
       " ('atomic', 0.4555259364535978),\n",
       " ('atoms', 0.4543258391303013),\n",
       " ('electron', 0.4415847215404407),\n",
       " ('electromagnetic', 0.4342020722504953),\n",
       " ('optical', 0.4341506741975586),\n",
       " ('physicist', 0.43388256332181196)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.most_similar(\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8945344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model[\"sailor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
