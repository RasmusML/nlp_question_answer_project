{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from models.answer_exists_models import *\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import compress_fasttext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5430bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: compress-fasttext in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (1.23.3)\n",
      "Requirement already satisfied: gensim>=4.0.0 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (1.9.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install compress-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ea285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0921e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the question is answered, then predict \"1\". Otherwise predict \"0\".\n",
    "def get_target(data):\n",
    "    answer_set = data['document_answer_region']\n",
    "    y = np.empty(answer_set.shape[0], dtype=np.int32)\n",
    "\n",
    "    for i, answer in enumerate(answer_set):\n",
    "        if type(answer) == Annotation_error and answer == Annotation_error.UNANSWERED: # @TODO: if we don't do the annotation stuff, then we can check for -1 here\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "13ad0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(raw_batch): #-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert pad_id != None\n",
    "    \n",
    "    question_sequence_lengths = [sample[1] for sample in raw_batch]\n",
    "    question_max_length = max(question_sequence_lengths)\n",
    "    question_ids = [(sample[0] + [pad_id] * (question_max_length - len(sample[0]))) for sample in raw_batch]\n",
    "    assert (all(len(i) == question_max_length for i in question_ids))\n",
    "    \n",
    "    document_sequence_lengths = [sample[3] for sample in raw_batch]\n",
    "    document_max_length = max(document_sequence_lengths)\n",
    "    document_ids = [(sample[2] + [pad_id] * (document_max_length - len(sample[2]))) for sample in raw_batch]\n",
    "    assert (all(len(i) == document_max_length for i in document_ids))\n",
    "    \n",
    "    targets = [sample[4] for sample in raw_batch]\n",
    "\n",
    "    return torch.tensor(question_ids), torch.tensor(question_sequence_lengths), torch.tensor(document_ids), torch.tensor(document_sequence_lengths), torch.tensor(targets, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "44e77b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9d2e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token, tok2vec):\n",
    "    assert OOV_id != None\n",
    "\n",
    "    try:\n",
    "        id = tok2vec.get_index(token)\n",
    "    except:\n",
    "        id = OOV_id # OOV\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4153ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(question_tokens, document_tokens, tok2vec, question_max_length=512, document_max_length=1024):\n",
    "    question_ids = [token_to_id(token, tok2vec) for token in question_tokens[:question_max_length]]\n",
    "    document_ids = [token_to_id(token, tok2vec) for token in document_tokens[:document_max_length]]\n",
    "    return question_ids, len(question_ids), document_ids, len(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "10bbee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweredDatasetReader(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tok2vec):\n",
    "        self.df = df\n",
    "        self.tok2vec = tok2vec\n",
    "        self.targets = get_target(df)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.values[idx]\n",
    "        question_tokens = row[1]\n",
    "        docment_tokens = row[3]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        question_ids, question_lengths, document_ids, document_lengths = prepare_sample(question_tokens, docment_tokens, self.tok2vec)\n",
    "        \n",
    "        return question_ids, question_lengths, document_ids, document_lengths, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "83241085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, n_classes: int = 2):\n",
    "        super(TinyNetwork1, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(pretrained_embeddings),\n",
    "            nn.Linear(pretrained_embeddings.shape[1], n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        return self.net.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6515fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork2(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 100, n_classes: int = 2):\n",
    "        super(TinyNetwork2, self).__init__()\n",
    "\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1)\n",
    "        self.biLSTM = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, 1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*lstm_dim, n_classes)\n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # b x sl x emb_dim\n",
    "        embeds = self.embedding(inputs)\n",
    "    \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(embeds, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence (b x sl x 2*lstm_dim)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # representation of the last lstm unit (b x 2*lstm_dim)\n",
    "        # Max element-wise over all hidden units\n",
    "        ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        # (b x n_classes)\n",
    "        logits = self.linear(ff_in)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7713898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork3(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 100, n_classes: int = 2):\n",
    "        super(TinyNetwork3, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(2*lstm_dim, n_classes)\n",
    "\n",
    "    def forward(self, q_ids, q_lengths, d_ids, d_lengths):\n",
    "        q_embeds = self.word_embeddings(q_ids)\n",
    "        lstm_out, _ = self.lstm(q_embeds)\n",
    "        q_ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        d_embeds = self.word_embeddings(d_ids)\n",
    "        lstm_out, _ = self.lstm(d_embeds)\n",
    "        d_ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        ff_in = torch.cat((q_ff_in, d_ff_in), dim=-1)\n",
    "        \n",
    "        logits = self.linear(ff_in)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "985c1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_matrix(embed_matrix):\n",
    "    embed_dim = embed_matrix.shape[1]\n",
    "    pad = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    OOV = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    embed_pre = (torch.tensor(embed_matrix, dtype=torch.float32))\n",
    "    \n",
    "    OOV_id = embed_pre.shape[0]           \n",
    "    pad_id = embed_pre.shape[0] + 1\n",
    "                 \n",
    "    return torch.vstack((embed_pre, pad, OOV)), pad_id, OOV_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "721fe0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cd5d9bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fcfbc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNetwork3(\n",
      "  (word_embeddings): Embedding(20002, 300)\n",
      "  (lstm): LSTM(300, 100, batch_first=True)\n",
      "  (linear): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tok2vec = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"fasttext-en-mini\")\n",
    "pretrained_embeddings, pad_id, OOV_id = prepare_embedding_matrix(tok2vec.get_normed_vectors())\n",
    "\n",
    "model = TinyNetwork3(pretrained_embeddings)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "433ccaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2])\n",
      "Output probabilities:\n",
      "[[0.02886636 0.00053837]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "q = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "q_lengths = torch.tensor([3], dtype=torch.int)\n",
    "d = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "d_lengths = torch.tensor([3], dtype=torch.int)\n",
    "\n",
    "out = model.forward(q, q_lengths, d, d_lengths)\n",
    "\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output probabilities:\\n{out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4699ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters()) #, weight_decay=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "73f64d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(token_ids, sequence_lengths):\n",
    "    actual_length = torch.sum(sequence_lengths)\n",
    "    total_length = np.sum([len(question) for question in token_ids])\n",
    "    \n",
    "    OOV_count = np.sum([torch.sum(question == OOV_id) for question in token_ids])\n",
    "    print(\"OOV: {}/{}\".format(OOV_count, total_length))\n",
    "    \n",
    "    pad_count = np.sum([torch.sum(question == pad_id) for question in token_ids])\n",
    "    print(\"PAD: {}/{}\".format(pad_count, total_length))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0e22e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [01:53<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6568851856322124 0.6058960686682521\n",
      "0.6908203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [01:44<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5549927073819884 0.7269619500594531\n",
      "0.7415364583333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [01:44<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5095011471160527 0.7577521180142688\n",
      "0.746484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [01:42<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47915023051459216 0.7783841780618311\n",
      "0.7462890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [01:47<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4626167619536663 0.785499962841855\n",
      "0.7609375\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "train_dataset = QuestionAnsweredDatasetReader(train_en, tok2vec)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_dataset = QuestionAnsweredDatasetReader(validation_en, tok2vec)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    \n",
    "    for q_ids, q_lengths, d_ids, d_lengths, targets in tqdm(train_dataloader):\n",
    "        outputs = model(q_ids, q_lengths, d_ids, d_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #for p in list(model.word_embeddings.parameters()):\n",
    "        #    print(p.grad)\n",
    "\n",
    "        predictions = outputs.max(1)[1]\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        \n",
    "        loss_epoch.append(loss.item())\n",
    "        accuracy_epoch.append(accuracy)\n",
    "        \n",
    "    mean_loss = np.mean(loss_epoch)\n",
    "    train_losses.append(mean_loss)\n",
    "    \n",
    "    mean_acc = np.mean(accuracy_epoch)\n",
    "    train_accuracies.append(mean_acc)\n",
    "    \n",
    "    print(mean_loss, mean_acc)\n",
    "\n",
    "    ### Evaluate validation\n",
    "    model.eval()\n",
    "\n",
    "    accuracy_epoch = []\n",
    "    for q_ids, q_lengths, d_ids, d_lengths, targets in val_dataloader:\n",
    "        model.eval()\n",
    "        \n",
    "        outputs = model(q_ids, q_lengths, d_ids, d_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        predictions = outputs.max(1)[1]\n",
    "\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        accuracy_epoch.append(accuracy)\n",
    "        \n",
    "    mean_acc = np.mean(accuracy_epoch)\n",
    "    print(mean_acc)\n",
    "    \n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "586efe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Updates'), Text(0, 0.5, 'Loss'))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = np.arange(len(train_losses))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accuracies)\n",
    "#plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dfa853f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6625496868429512,\n",
       " [],\n",
       " 0.6160800040795885,\n",
       " [],\n",
       " 0.5222359697880417,\n",
       " [],\n",
       " 0.48120774148867046,\n",
       " [],\n",
       " 0.4642049968242645,\n",
       " []]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f03be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8153295516967773, 'cupcake'),\n",
       " (0.7879170179367065, 'cookie-'),\n",
       " (0.7691606879234314, 'cookies'),\n",
       " (0.760718047618866, 'cake'),\n",
       " (0.7547473907470703, 'popover'),\n",
       " (0.7430337071418762, 'non-cookie'),\n",
       " (0.7359125018119812, 'cakepop'),\n",
       " (0.7353752255439758, 'muffin'),\n",
       " (0.7349858283996582, 'cookie.'),\n",
       " (0.7314939498901367, 'cookie.The')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "627a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31058589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> confection.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], confection.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], enable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      enable (Iterable[str]): Names of pipeline components to enable. All other\n",
      " |          pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import compress_fasttext\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from compress_fasttext.feature_extraction import FastTextTransformer\n",
    "\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-en-mini')\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    FastTextTransformer(model=small_model), \n",
    "    LogisticRegression()\n",
    ").fit(\n",
    "    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],\n",
    "    [1, 1, 1, 0, 0, 0]\n",
    ")\n",
    "classifier.predict(['jet', 'train', 'cake', 'apple'])\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b97420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "449e1dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c219863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15784"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_index(\"slap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3356762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quantum', 0.5923356945161),\n",
       " ('physics', 0.4987263608889812),\n",
       " ('computational', 0.4833229306372649),\n",
       " ('cosmic', 0.46287664812730667),\n",
       " ('atomic', 0.4555259364535978),\n",
       " ('atoms', 0.4543258391303013),\n",
       " ('electron', 0.4415847215404407),\n",
       " ('electromagnetic', 0.4342020722504953),\n",
       " ('optical', 0.4341506741975586),\n",
       " ('physicist', 0.43388256332181196)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.most_similar(\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af44447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model[\"sailor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78032967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tok2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00265f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
