{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "is_in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if is_in_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/KU_NLP\n",
    "  !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2be9eb",
   "metadata": {},
   "source": [
    "# 2. Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d4ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from models.answer_exists_models import *\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import compress_fasttext\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60b183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: compress-fasttext in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (1.23.3)\n",
      "Requirement already satisfied: gensim>=4.0.0 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from compress-fasttext) (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\programdata\\anaconda3\\envs\\ku_nlp\\lib\\site-packages (from gensim>=4.0.0->compress-fasttext) (0.29.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install compress-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e2735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c299d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation_error(Enum):\n",
    "    UNANSWERED = -1\n",
    "    BAD_TOKENIZATION_OR_DATA = -2\n",
    "    IGNORED = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7704b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_set = \"data/train_set_stanza.pkl\"\n",
    "path_validation_set = \"data/validation_set_stanza.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b72f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_pickle(path_train_set)\n",
    "validation_set = pd.read_pickle(path_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "837acc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = train_set[train_set[\"language\"] == \"english\"]\n",
    "train_fi = train_set[train_set[\"language\"] == \"finnish\"]\n",
    "train_ja = train_set[train_set[\"language\"] == \"japanese\"]\n",
    "\n",
    "validation_en = validation_set[validation_set[\"language\"] == \"english\"]\n",
    "validation_fi = validation_set[validation_set[\"language\"] == \"finnish\"]\n",
    "validation_ja = validation_set[validation_set[\"language\"] == \"japanese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c83495",
   "metadata": {},
   "source": [
    "# 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b501af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document</th>\n",
       "      <th>document_answer_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[Quantum, field, theory]</td>\n",
       "      <td>[Quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, the, first, Nobel, prize, winner, f...</td>\n",
       "      <td>[List, of, Nobel, laureates, in, Literature]</td>\n",
       "      <td>[The, Nobel, Prize, in, Literature, (, Swedish...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english</td>\n",
       "      <td>[When, is, the, dialectical, method, used, ?]</td>\n",
       "      <td>[Dialectic]</td>\n",
       "      <td>[Dialectic, or, dialectics, (, Greek, :, διαλε...</td>\n",
       "      <td>(27, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, invented, Hangul, ?]</td>\n",
       "      <td>[Origin, of, Hangul]</td>\n",
       "      <td>[Hangul, was, personally, created, and, promul...</td>\n",
       "      <td>(16, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, do, Grasshoppers, eat, ?]</td>\n",
       "      <td>[Grasshopper]</td>\n",
       "      <td>[Grasshoppers, are, plant, -, eaters, ,, with,...</td>\n",
       "      <td>(1, 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, was, Neil, Brooks, ', fastest, recorded...</td>\n",
       "      <td>[Swimming, at, the, 1980, Summer, Olympics, –,...</td>\n",
       "      <td>[The, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, are, the, three, most, important, easter...</td>\n",
       "      <td>[Eastern, philosophy]</td>\n",
       "      <td>[Sāmkhya, is, a, dualist, philosophical, tradi...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, was, costume, designer, for, the, first,...</td>\n",
       "      <td>[John, Mollo]</td>\n",
       "      <td>[Mollo, was, surprised, by, the, success, of, ...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>english</td>\n",
       "      <td>[Who, developed, the, first, thermonuclear, we...</td>\n",
       "      <td>[History, of, nuclear, weapons]</td>\n",
       "      <td>[In, the, end, ,, President, Truman, made, the...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>english</td>\n",
       "      <td>[What, is, the, population, of, Mahwah, ,, NJ, ?]</td>\n",
       "      <td>[Mahwah, ,, New, Jersey]</td>\n",
       "      <td>[The, previous, mayor, ,, Bill, Laforet, faced...</td>\n",
       "      <td>Annotation_error.UNANSWERED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     language                                           question  \\\n",
       "0     english  [When, was, quantum, field, theory, developed, ?]   \n",
       "1     english  [Who, was, the, first, Nobel, prize, winner, f...   \n",
       "2     english      [When, is, the, dialectical, method, used, ?]   \n",
       "3     english                         [Who, invented, Hangul, ?]   \n",
       "4     english                   [What, do, Grasshoppers, eat, ?]   \n",
       "...       ...                                                ...   \n",
       "7384  english  [What, was, Neil, Brooks, ', fastest, recorded...   \n",
       "7385  english  [Who, are, the, three, most, important, easter...   \n",
       "7386  english  [Who, was, costume, designer, for, the, first,...   \n",
       "7387  english  [Who, developed, the, first, thermonuclear, we...   \n",
       "7388  english  [What, is, the, population, of, Mahwah, ,, NJ, ?]   \n",
       "\n",
       "                                         document_title  \\\n",
       "0                              [Quantum, field, theory]   \n",
       "1          [List, of, Nobel, laureates, in, Literature]   \n",
       "2                                           [Dialectic]   \n",
       "3                                  [Origin, of, Hangul]   \n",
       "4                                         [Grasshopper]   \n",
       "...                                                 ...   \n",
       "7384  [Swimming, at, the, 1980, Summer, Olympics, –,...   \n",
       "7385                              [Eastern, philosophy]   \n",
       "7386                                      [John, Mollo]   \n",
       "7387                    [History, of, nuclear, weapons]   \n",
       "7388                           [Mahwah, ,, New, Jersey]   \n",
       "\n",
       "                                               document  \\\n",
       "0     [Quantum, field, theory, naturally, began, wit...   \n",
       "1     [The, Nobel, Prize, in, Literature, (, Swedish...   \n",
       "2     [Dialectic, or, dialectics, (, Greek, :, διαλε...   \n",
       "3     [Hangul, was, personally, created, and, promul...   \n",
       "4     [Grasshoppers, are, plant, -, eaters, ,, with,...   \n",
       "...                                                 ...   \n",
       "7384  [The, medley, relay, was, scheduled, in, the, ...   \n",
       "7385  [Sāmkhya, is, a, dualist, philosophical, tradi...   \n",
       "7386  [Mollo, was, surprised, by, the, success, of, ...   \n",
       "7387  [In, the, end, ,, President, Truman, made, the...   \n",
       "7388  [The, previous, mayor, ,, Bill, Laforet, faced...   \n",
       "\n",
       "           document_answer_region  \n",
       "0                        (26, 26)  \n",
       "1                        (12, 13)  \n",
       "2                        (27, 49)  \n",
       "3                        (16, 18)  \n",
       "4                         (1, 37)  \n",
       "...                           ...  \n",
       "7384  Annotation_error.UNANSWERED  \n",
       "7385  Annotation_error.UNANSWERED  \n",
       "7386  Annotation_error.UNANSWERED  \n",
       "7387  Annotation_error.UNANSWERED  \n",
       "7388  Annotation_error.UNANSWERED  \n",
       "\n",
       "[7389 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6766f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the question is answered, then predict \"1\". Otherwise predict \"0\".\n",
    "def get_target(data):\n",
    "    answer_set = data['document_answer_region']\n",
    "    y = np.empty(answer_set.shape[0], dtype=np.int32)\n",
    "\n",
    "    for i, answer in enumerate(answer_set):\n",
    "        if type(answer) == Annotation_error and answer == Annotation_error.UNANSWERED: # @TODO: if we don't do the annotation stuff, then we can check for -1 here\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "457ce3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(raw_batch): #-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    global pad_id # @TODO: cleanup\n",
    "    \n",
    "    targets = [sample[2] for sample in raw_batch]\n",
    "    sequence_lengths = [sample[1] for sample in raw_batch]\n",
    "    \n",
    "    max_length = max(sequence_lengths)\n",
    "    \n",
    "    # Pad all of the input samples to the max length\n",
    "    token_ids = [(sample[0] + [pad_id] * (max_length - len(sample[0]))) for sample in raw_batch]\n",
    "    \n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in token_ids))\n",
    "    \n",
    "    return torch.tensor(token_ids), torch.tensor(sequence_lengths), torch.tensor(targets, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46f51350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_id(token, tok2vec):\n",
    "    global OOV_id # @TODO: cleanup\n",
    "    \n",
    "    try:\n",
    "        id = tok2vec.get_index(token)\n",
    "    except:\n",
    "        id = OOV_id # OOV\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2d55d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(question_tokens, tok2vec, max_length=512):\n",
    "    token_ids = [token_to_id(token, tok2vec) for token in question_tokens[:max_length]]\n",
    "    return token_ids, len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bc6341c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweredDatasetReader(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tok2vec):\n",
    "        self.df = df\n",
    "        self.tok2vec = tok2vec\n",
    "        self.targets = get_target(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.values[idx]\n",
    "        question_tokens = row[1]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        token_ids, seq_length = prepare_sample(question_tokens, self.tok2vec)\n",
    "        \n",
    "        return token_ids, seq_length, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9991a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = QuestionAnsweredDatasetReader(train_en, tok2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "601e6070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([264, 22, 9677, 765, 2080, 1247, 41], 7, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eeb2ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuestionAnsweredDatasetReader(train_en[:100], tok2vec)\n",
    "\n",
    "batch_size=8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e06fea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, n_classes: int = 2):\n",
    "        super(TinyNetwork1, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(pretrained_embeddings),\n",
    "            nn.Linear(pretrained_embeddings.shape[1], n_classes),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.net.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7abb3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNetwork2(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embeddings: torch.tensor, lstm_dim: int = 100, n_classes: int = 2):\n",
    "        super(TinyNetwork2, self).__init__()\n",
    "\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.biLSTM = nn.LSTM(pretrained_embeddings.shape[1], lstm_dim, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*lstm_dim, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # b x sl x emb_dim\n",
    "        embeds = self.embedding(inputs)\n",
    "    \n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(embeds, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, _hidden = self.biLSTM(lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence (b x sl x 2*lstm_dim)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # representation of the last lstm unit (b x 2*lstm_dim)\n",
    "        n_batch = lstm_out.shape[0]\n",
    "        input_ends = input_lengths-1\n",
    "        ff_in = torch.vstack([lstm_out[i, input_ends[i], :] for i in range(n_batch)])\n",
    "        # Alternative: Max element-wise over all hidden units\n",
    "        # ff_in = torch.max(lstm_out, 1)[0]\n",
    "        \n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.linear(ff_in)\n",
    "        \n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ffe1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1: [.1, .4, .6], [.!, .!, .!], [.0, .0, .0], [.0, .0, .0],\n",
    "# b2: [.1, .4, .6], [.1, .4, .6], [.!, .!, .!], [.0, .0, .0],\n",
    "# b3: [.1, .4, .6], [.1, .4, .6], [.1, .4, .6], [.!, .!, .!],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5a9fc226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2],[2,3],[6,4],[4,5]], [[5,6],[6,7],[7,8],[8,9]]])\n",
    "indices = [0, 2]\n",
    "torch.vstack([a[i,indices[i],:] for i in range(len(indices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d0b45857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embedding_matrix(embed_matrix):\n",
    "    embed_dim = embed_matrix.shape[1]\n",
    "    pad = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    OOV = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "    embed_pre = (torch.tensor(embed_matrix, dtype=torch.float32))\n",
    "    \n",
    "    pad_id = embed_pre.shape[1]\n",
    "    OOV_id = embed_pre.shape[1] + 1           \n",
    "                 \n",
    "    return torch.vstack((embed_pre, pad, OOV)), pad_id, OOV_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c165941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6a4b71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNetwork2(\n",
      "  (embedding): Embedding(20002, 300)\n",
      "  (biLSTM): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tok2vec = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"fasttext-en-mini\")\n",
    "pretrained_embeddings, pad_id, OOV_id = prepare_embedding_matrix(tok2vec.get_normed_vectors())\n",
    "\n",
    "model = TinyNetwork2(pretrained_embeddings)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "57e6e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2])\n",
      "Output probabilities:\n",
      "[[0.52015305 0.47984695]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with dummy data\n",
    "inputs = torch.tensor([[42, 1, 8, 0]], dtype=torch.int)\n",
    "input_lengths = torch.tensor([3], dtype=torch.int)\n",
    "out = model.forward(inputs, input_lengths)\n",
    "\n",
    "print(\"Output shape:\", out.size())\n",
    "print(f\"Output probabilities:\\n{out.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "63fa122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "081b4caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "train_dataset = QuestionAnsweredDatasetReader(train_en, tok2vec)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9435b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Accuracy: 0.48\n",
      "Epoch 2/5\n",
      "Accuracy: 0.45\n",
      "Epoch 3/5\n",
      "Accuracy: 0.55\n",
      "Epoch 4/5\n",
      "Accuracy: 0.48\n",
      "Epoch 5/5\n",
      "Accuracy: 0.45\n",
      "Finished training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Updates'), Text(0, 0.5, 'Acc'))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "train_dataset = QuestionAnsweredDatasetReader(train_en, tok2vec)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "train_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "    for token_ids, sequence_lengths, targets in train_dataloader:\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass.\n",
    "        outputs = model(token_ids, sequence_lengths)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Do prediction\n",
    "        predictions = outputs.max(1)[1]\n",
    "\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        train_acc.append(accuracy)\n",
    "          \n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "        \n",
    "\n",
    "print(\"Finished training.\")\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7788e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "accuracy: 0.5\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_sentences = torch.tensor([[42, 1, 0], [42, 1, 0], [18, 14, 0], [18, 1, 2]], dtype=torch.int32)\n",
    "train_lengths = torch.tensor([2, 2, 2, 3], dtype=torch.int32)\n",
    "train_target = torch.tensor([1, 1, 0, 0])\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = train_sentences\n",
    "    lengths = train_lengths\n",
    "    targets = train_target\n",
    "    \n",
    "    # Forward pass.\n",
    "    outputs = model(inputs, lengths)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # Backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    # Compute accuracy.\n",
    "    predictions = outputs.max(1)[1]\n",
    "    \n",
    "    #print(outputs)\n",
    "    #print(predictions)\n",
    "    \n",
    "    print(\"accuracy: {}\".format(accuracy_score(targets, predictions)))\n",
    "        \n",
    "\n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82333316",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b24b9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afb69",
   "metadata": {},
   "source": [
    "# 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc59626",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_dimension()\n",
    "ft.get_word_vector('king').shape\n",
    "\n",
    "fasttext.util.reduce_model(ft, 80)\n",
    "ft.get_dimension()\n",
    "\n",
    "\"asdasdsad\" in ft.words\n",
    "ft.get_nearest_neighbors('cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfb2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3196d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"old\"].vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31058589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[confection.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> confection.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], confection.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], enable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      enable (Iterable[str]): Names of pipeline components to enable. All other\n",
      " |          pipes will be disabled (and can be enabled using `nlp.enable_pipe`).\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a6ba4",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/text/issues/1350\n",
    "https://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1870b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import compress_fasttext\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from compress_fasttext.feature_extraction import FastTextTransformer\n",
    "\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-en-mini')\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    FastTextTransformer(model=small_model), \n",
    "    LogisticRegression()\n",
    ").fit(\n",
    "    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],\n",
    "    [1, 1, 1, 0, 0, 0]\n",
    ")\n",
    "classifier.predict(['jet', 'train', 'cake', 'apple'])\n",
    "# array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "141aca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CompressedFastTextKeyedVectors in module compress_fasttext.compress object:\n",
      "\n",
      "class CompressedFastTextKeyedVectors(gensim.models.fasttext.FastTextKeyedVectors)\n",
      " |  CompressedFastTextKeyedVectors(*args, **kwargs)\n",
      " |  \n",
      " |  This class extends FastTextKeyedVectors by fixing several issues:\n",
      " |  - index2word of a freshly created model is initialized from its vocab\n",
      " |  - the model does not keep heavy and useless vectors_ngrams_norm\n",
      " |  - word_vec() method with use_norm applies normalization in the right place\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CompressedFastTextKeyedVectors\n",
      " |      gensim.models.fasttext.FastTextKeyedVectors\n",
      " |      gensim.models.keyedvectors.KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Vectors and vocab for :class:`~gensim.models.fasttext.FastText`.\n",
      " |      \n",
      " |      Implements significant parts of the FastText algorithm.  For example,\n",
      " |      the :func:`word_vec` calculates vectors for out-of-vocabulary (OOV)\n",
      " |      entities.  FastText achieves this by keeping vectors for ngrams:\n",
      " |      adding the vectors for the ngrams of an entity yields the vector for the\n",
      " |      entity.\n",
      " |      \n",
      " |      Similar to a hashmap, this class keeps a fixed number of buckets, and\n",
      " |      maps all ngrams to buckets using a hash function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_size : int\n",
      " |          The dimensionality of all vectors.\n",
      " |      min_n : int\n",
      " |          The minimum number of characters in an ngram\n",
      " |      max_n : int\n",
      " |          The maximum number of characters in an ngram\n",
      " |      bucket : int\n",
      " |          The number of buckets.\n",
      " |      count : int, optional\n",
      " |          If provided, vectors will be pre-allocated for at least this many vectors. (Otherwise\n",
      " |          they can be added later.)\n",
      " |      dtype : type, optional\n",
      " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
      " |          another type is provided here.\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      vectors_vocab : np.array\n",
      " |          Each row corresponds to a vector for an entity in the vocabulary.\n",
      " |          Columns correspond to vector dimensions. When embedded in a full\n",
      " |          FastText model, these are the full-word-token vectors updated\n",
      " |          by training, whereas the inherited vectors are the actual per-word\n",
      " |          vectors synthesized from the full-word-token and all subword (ngram)\n",
      " |          vectors.\n",
      " |      vectors_ngrams : np.array\n",
      " |          A vector for each ngram across all entities in the vocabulary.\n",
      " |          Each row is a vector that corresponds to a bucket.\n",
      " |          Columns correspond to vector dimensions.\n",
      " |      buckets_word : list of np.array\n",
      " |          For each key (by its index), report bucket slots their subwords map to.\n",
      " |  \n",
      " |  adjust_vectors(self)\n",
      " |      Adjust the vectors for words in the vocabulary.\n",
      " |      \n",
      " |      The adjustment composes the trained full-word-token vectors with\n",
      " |      the vectors of the subword ngrams, matching the Facebook reference\n",
      " |      implementation behavior.\n",
      " |  \n",
      " |  fill_norms(self, force=False)\n",
      " |      Ensure per-vector norms are available.\n",
      " |      \n",
      " |      Any code which modifies vectors should ensure the accompanying norms are\n",
      " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |  \n",
      " |  recalc_char_ngram_buckets(self)\n",
      " |      Scan the vocabulary, calculate ngrams and their hashes, and cache the list of ngrams for each known word.\n",
      " |  \n",
      " |  update_index2word(self)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all ngrams not in vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastTextKeyedVectors` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastTextKeyedVectors` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.fasttext.FastTextKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Check if `word` or any character ngrams in `word` are present in the vocabulary.\n",
      " |      A vector for the word is guaranteed to exist if current method returns True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          True if `word` or any character ngrams in `word` are present in the vocabulary, False otherwise.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      This method **always** returns True with char ngrams, because of the way FastText works.\n",
      " |      \n",
      " |      If you want to check if a word is an in-vocabulary term, use this instead:\n",
      " |      \n",
      " |      .. pycon:\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> model = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>> 'steamtrain' in model.wv.key_to_index  # If False, is an OOV term\n",
      " |          False\n",
      " |  \n",
      " |  get_sentence_vector(self, sentence)\n",
      " |      Get a single 1-D vector representation for a given `sentence`.\n",
      " |      This function is workalike of the official fasttext's get_sentence_vector().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : list of (str or int)\n",
      " |          list of words specified by string or int ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          1-D numpy array representation of the `sentence`.\n",
      " |  \n",
      " |  get_vector(self, word, norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word.\n",
      " |      norm : bool, optional\n",
      " |          If True, resulting vector will be L2-normalized (unit Euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word and all its ngrams not in vocabulary.\n",
      " |  \n",
      " |  init_post_load(self, fb_vectors)\n",
      " |      Perform initialization after loading a native Facebook model.\n",
      " |      \n",
      " |      Expects that the vocabulary (self.key_to_index) has already been initialized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fb_vectors : np.array\n",
      " |          A matrix containing vectors for all the entities, including words\n",
      " |          and ngrams.  This comes directly from the binary model.\n",
      " |          The order of the vectors must correspond to the indices in\n",
      " |          the vocabulary.\n",
      " |  \n",
      " |  resize_vectors(self, seed=0)\n",
      " |      Make underlying vectors match 'index_to_key' size; random-initialize any new rows.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastTextKeyedVectors.load`\n",
      " |          Load object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, key_or_keys)\n",
      " |      Get vector representation of `key_or_keys`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key_or_keys : {str, list of str, int, list of int}\n",
      " |          Requested key or list-of-keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, keys, weights)\n",
      " |      Add keys and theirs vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      \n",
      " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
      " |      with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : {str, int, list of (str or int)}\n",
      " |          keys specified by their string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vector(self, key, vector)\n",
      " |      Add one new vector at the given key, into existing slot if available.\n",
      " |      \n",
      " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
      " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key: str\n",
      " |          Key identifier of the added vector.\n",
      " |      vector: numpy.ndarray\n",
      " |          1D numpy array with the vector values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
      " |          ``self.index_to_key[result] == key``.\n",
      " |  \n",
      " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
      " |      Append keys and their vectors in a manual way.\n",
      " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of (str or int)\n",
      " |          Keys specified by string or int ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  allocate_vecattrs(self, attrs=None, types=None)\n",
      " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
      " |      \n",
      " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
      " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
      " |      So this allocation targets that size.\n",
      " |  \n",
      " |  closer_than(self, key1, key2)\n",
      " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two keys.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which key from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The key further away from the mean of all keys.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      similarity_function : str, optional\n",
      " |          Function name used for similarity calculation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_index(self, key, default=None)\n",
      " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
      " |      backing vectors array.\n",
      " |  \n",
      " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
      " |      Get the mean vector for a given list of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      keys : list of (str or int or ndarray)\n",
      " |          Keys specified by string or int ids or numpy array.\n",
      " |      weights : list of float or numpy.ndarray, optional\n",
      " |          1D array of same size of `keys` specifying the weight for each key.\n",
      " |      pre_normalize : bool, optional\n",
      " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
      " |          If False, individual keyvector will not be normalized.\n",
      " |      post_normalize: bool, optional\n",
      " |          Flag indicating whether to normalize the final mean vector.\n",
      " |          If True, normalized mean vector will be return.\n",
      " |      ignore_missing : bool, optional\n",
      " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      numpy.ndarray\n",
      " |          Mean vector for the list of keys.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      \n",
      " |      ValueError\n",
      " |          If the size of the list of `keys` and `weights` doesn't match.\n",
      " |      KeyError\n",
      " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
      " |  \n",
      " |  get_normed_vectors(self)\n",
      " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
      " |      \n",
      " |      To see which key corresponds to which vector = which array row, refer\n",
      " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray:\n",
      " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
      " |          along the rows (key vectors).\n",
      " |  \n",
      " |  get_vecattr(self, key, attr)\n",
      " |      Get attribute value associated with given key.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Vector key for which to fetch the attribute value.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to fetch for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      object\n",
      " |          Value of the additional attribute fetched for the given key.\n",
      " |  \n",
      " |  has_index_for(self, key)\n",
      " |      Can this model return a single index for this key?\n",
      " |      \n",
      " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
      " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
      " |      simple `word in wv` (`__contains__()`) check but False for this\n",
      " |      more-specific check.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given keys and the vectors for each key in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
      " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
      " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      clip_start : int\n",
      " |          Start clipping index.\n",
      " |      clip_end : int\n",
      " |          End clipping index.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
      " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
      " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int or None, optional\n",
      " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
      " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
      " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_to_given(self, key1, keys_list)\n",
      " |      Get the `key` from `keys_list` most similar to `key1`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of keys.\n",
      " |      ws2: list of str\n",
      " |          Sequence of keys.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  rank(self, key1, key2)\n",
      " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
      " |  \n",
      " |  rank_by_centrality(self, words, use_norm=True)\n",
      " |      Rank the given words by similarity to the centroid of all the words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of keys.\n",
      " |      use_norm : bool, optional\n",
      " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (float, str)\n",
      " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          File path to save the vectors to.\n",
      " |      fvocab : str, optional\n",
      " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Explicitly specify total number of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |      write_header : bool, optional\n",
      " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
      " |          This is the format used by e.g. gloVe vectors.\n",
      " |      prefix : str, optional\n",
      " |          String to prepend in front of each stored word. Default = no prefix.\n",
      " |      append : bool, optional\n",
      " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
      " |      sort_attr : str, optional\n",
      " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
      " |  \n",
      " |  set_vecattr(self, key, attr, val)\n",
      " |      Set attribute associated with the given key to value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      key : str\n",
      " |          Store the attribute for this vector key.\n",
      " |      attr : str\n",
      " |          Name of the additional attribute to store for the given key.\n",
      " |      val : object\n",
      " |          Value of the additional attribute to store for the given key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      \n",
      " |      None\n",
      " |  \n",
      " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar keys by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all keys are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all keys are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Compatibility alias for similar_by_key().\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two keys.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input key.\n",
      " |      w2 : str\n",
      " |          Input key.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_unseen_docs(self, *args, **kwargs)\n",
      " |  \n",
      " |  sort_by_descending_frequency(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  unit_normalize_all(self)\n",
      " |      Destructively scale all vectors to unit-length.\n",
      " |      \n",
      " |      You cannot sensibly continue training after such a step.\n",
      " |  \n",
      " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
      " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
      " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
      " |      corpus:\n",
      " |      \n",
      " |      >>> from collections import Counter\n",
      " |      >>> import itertools\n",
      " |      >>>\n",
      " |      >>> from gensim.models import FastText\n",
      " |      >>> from gensim.test.utils import datapath, common_texts\n",
      " |      >>>\n",
      " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
      " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
      " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
      " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
      " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
      " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : iterable\n",
      " |          The keys that will be vectorized.\n",
      " |      allow_inference : bool, optional\n",
      " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
      " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
      " |      copy_vecattrs : bool, optional\n",
      " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
      " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
      " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
      " |          you should set `allow_inference=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Vectors for all the given keys.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2, norm=True)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      norm : boolean\n",
      " |          Normalize all word vectors to unit length before computing the distance?\n",
      " |          Defaults to True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  words_closer_than(self, word1, word2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
      " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      no_header : bool, optional\n",
      " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
      " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
      " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
      " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.keyedvectors.KeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  index2word\n",
      " |  \n",
      " |  vectors_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cbdc56fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d7baf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15784"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.get_index(\"slap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2524c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quantum', 0.5923356945161),\n",
       " ('physics', 0.4987263608889812),\n",
       " ('computational', 0.4833229306372649),\n",
       " ('cosmic', 0.46287664812730667),\n",
       " ('atomic', 0.4555259364535978),\n",
       " ('atoms', 0.4543258391303013),\n",
       " ('electron', 0.4415847215404407),\n",
       " ('electromagnetic', 0.4342020722504953),\n",
       " ('optical', 0.4341506741975586),\n",
       " ('physicist', 0.43388256332181196)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.most_similar(\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "37b60b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model[\"sailor\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abcf28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
